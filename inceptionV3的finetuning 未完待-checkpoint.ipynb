{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hosilab\\anaconda2\\envs\\tf14\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   \n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InceptionV3(weights=r'.\\Desktop\\sample\\inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',include_top=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights=r'C:\\Users\\hosilab\\Desktop\\sample\\inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                         include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = [os.path.join(root,name)\n",
    "                    for root,dirs,files in sorted(os.walk(path))\n",
    "                    for name in files if os.path.splitext(name.lower())[-1] in ('.jpeg','.jpg')]\n",
    "nb_validation_samples = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''这种写法是错误的'''\n",
    "all_pic_dirs = [os.path.join(root,name)\n",
    "                for root,dirs,files in sorted(os.walk(path))\n",
    "                for name in files if '.jpeg'or'.jpg' in \n",
    "                os.path.splitext(name.lower())[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(all_pic_dirs) - set(nb_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffents(set(all_pic_dirs),set(nb_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 917578 images belonging to 3 classes.\n",
      "Found 160281 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      " 8055/71630 [==>...........................] - ETA: 14:09:54 - loss: 9.2385 - acc: 0.4267"
     ]
    }
   ],
   "source": [
    "classes = ['Forward3reward', 'Left3reward', 'Right3reward']\n",
    "# create the base pre-trained model\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "base_model = InceptionV3(weights=r'.\\Desktop\\sample\\inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                         include_top=False)\n",
    "# dimensions of our images.\n",
    "#Inception input size\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "top_layers_checkpoint_path = 'cp.top.best.hdf5'\n",
    "fine_tuned_checkpoint_path = 'cp.fine_tuned.best.hdf5'\n",
    "new_extended_inception_weights = '20180913final_weights.hdf5'\n",
    "\n",
    "train_data_dir =r'C:\\Users\\hosilab\\Desktop\\ps\\agent'\n",
    "validation_data_dir = r'C:\\Users\\hosilab\\Desktop\\sample\\agent'\n",
    "\n",
    "nb_train_samples = 2148904\n",
    "nb_validation_samples = 160281\n",
    "\n",
    "top_epochs = 50\n",
    "fit_epochs = 50\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(data_format=\"channels_last\")(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- we have 2 classes\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "if os.path.exists(top_layers_checkpoint_path):\n",
    "    model.load_weights(top_layers_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + top_layers_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    data_format=\"channels_last\",\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    classes=classes,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    classes=classes,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_top = ModelCheckpoint(top_layers_checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Save the TensorBoard logs.\n",
    "tb = TensorBoard(log_dir='./logs0904', histogram_freq=1, write_graph=True, write_images=True)#没有验证集的时候 histogram_freq=0,默认1\n",
    "#https://github.com/aurora95/Keras-FCN/issues/50\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "#model.fit_generator(...)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=top_epochs,\n",
    "    #samples_per_epoch=nb_train_samples // batch_size,#老版本\n",
    "    #nb_epoch=top_epochs,# 老版本\n",
    "    #nb_val_samples=nb_validation_samples // batch_size,#老版本\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[mc_top, tb])\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_fit = ModelCheckpoint(fine_tuned_checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "if os.path.exists(fine_tuned_checkpoint_path):\n",
    "    model.load_weights(fine_tuned_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + fine_tuned_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "#model.fit_generator(...)\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        #samples_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=fit_epochs,\n",
    "        #nb_epoch=fit_epochs,#老版本\n",
    "        #validation_spilt=0.1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size,\n",
    "        #nb_val_samples=nb_validation_samples // batch_size,\n",
    "        callbacks=[mc_fit, tb])\n",
    "\n",
    "model.save_weights(new_extended_inception_weights)\n",
    "#2386/2387 [============================>.] - ETA: 0s - loss: 7.9823 - acc: 0.5043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Forward3reward', 'Left3reward', 'Right3reward']\n",
    "# create the base pre-trained model\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "base_model = InceptionV3(weights=r'.\\Desktop\\sample\\inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', \n",
    "                         include_top=False)\n",
    "# dimensions of our images.\n",
    "#Inception input size\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "top_layers_checkpoint_path = 'cp.top.best.hdf5'\n",
    "fine_tuned_checkpoint_path = 'cp.fine_tuned.best.hdf5'\n",
    "new_extended_inception_weights = '20180913final_weights.hdf5'\n",
    "\n",
    "train_data_dir =r'C:\\Users\\hosilab\\Desktop\\ps\\agent'\n",
    "validation_data_dir = r'C:\\Users\\hosilab\\Desktop\\sample\\agent'\n",
    "\n",
    "nb_train_samples = 2148904\n",
    "nb_validation_samples = 160281\n",
    "\n",
    "top_epochs = 50\n",
    "fit_epochs = 50\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(data_format=\"channels_last\")(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- we have 2 classes\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "if os.path.exists(top_layers_checkpoint_path):\n",
    "    model.load_weights(top_layers_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + top_layers_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    data_format=\"channels_last\",\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#     validation_data_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical')\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_top = ModelCheckpoint(top_layers_checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Save the TensorBoard logs.\n",
    "tb = TensorBoard(log_dir='./logs0904', histogram_freq=0, write_graph=True, write_images=True)#没有验证集的时候 histogram_freq=0,默认1\n",
    "#https://github.com/aurora95/Keras-FCN/issues/50\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "#model.fit_generator(...)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples // batch_size,\n",
    "    nb_epoch=top_epochs,####################################################\n",
    "    nb_val_samples=nb_validation_samples // batch_size,\n",
    "    callbacks=[mc_top, tb])# validation_data=validation_generator,\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\\\n",
    "    print(i, layer.name)\n",
    "\n",
    "\n",
    "#Save the model after every epoch.\n",
    "mc_fit = ModelCheckpoint(fine_tuned_checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "if os.path.exists(fine_tuned_checkpoint_path):\n",
    "    model.load_weights(fine_tuned_checkpoint_path)\n",
    "    print (\"Checkpoint '\" + fine_tuned_checkpoint_path + \"' loaded.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "#model.fit_generator(...)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples // batch_size,\n",
    "    nb_epoch=fit_epochs,########################################################\n",
    "    validation_spilt=0.1,\n",
    "    #validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples // batch_size,\n",
    "    callbacks=[mc_fit, tb])\n",
    "\n",
    "model.save_weights(new_extended_inception_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
